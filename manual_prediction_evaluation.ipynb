{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import physio\n",
    "import matplotlib.pyplot as plt\n",
    "import neurokit2 as nk\n",
    "from customLib.preprocess import myConv1D, norm_min_max, invert_log_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "PATH = \"E:/ml-data/masters-thesis/myDataset/resp_prediction/mixed\"\n",
    "POST_PROCESS_PATH = \"E:/ml-data/masters-thesis/myDataset/resp_prediction/postprocess/mixed\"\n",
    "\n",
    "aidmed_resp_sampling_rate = 50\n",
    "aidmed_ecg_sampling_rate = 250\n",
    "\n",
    "files = os.listdir(PATH)\n",
    "print(len([x for x in files if \"targets.npy\" in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_local_extrema(x: np.array, y: np.array):\n",
    "    \"\"\"\n",
    "    Detects local maxima and minima using the second derivative test.\n",
    "    \n",
    "    Args:\n",
    "    - x: np.array, independent variable (e.g., time or position)\n",
    "    - y: np.array, dependent variable (e.g., function values)\n",
    "    \n",
    "    Returns:\n",
    "    - local_maxima: list of tuples (x, y) of local maxima points\n",
    "    - local_minima: list of tuples (x, y) of local minima points\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate the first and second derivatives using numpy's gradient function\n",
    "    dy_dx = np.gradient(y, x)   # First derivative\n",
    "    d2y_dx2 = np.gradient(dy_dx, x)  # Second derivative\n",
    "\n",
    "    # Step 2: Identify points where the first derivative crosses zero (potential extrema)\n",
    "    local_maxima = []\n",
    "    local_minima = []\n",
    "    \n",
    "    for i in range(1, len(dy_dx) - 1):\n",
    "        if np.sign(dy_dx[i-1]) != np.sign(dy_dx[i+1]):  # Derivative changes sign\n",
    "            if d2y_dx2[i] < 0:  # Negative second derivative => local maxima\n",
    "                local_maxima.append((x[i]))\n",
    "            elif d2y_dx2[i] > 0:  # Positive second derivative => local minima\n",
    "                local_minima.append((x[i]))\n",
    "    \n",
    "    return np.array(local_maxima), np.array(local_minima)\n",
    "\n",
    "def detect_error_indices(target, prediction, neighbourhood_size, window_length):\n",
    "    # Create a binary array where target peak indices are set to 1\n",
    "    target_prob = np.zeros(window_length)\n",
    "    target_prob[target] = 1\n",
    "\n",
    "    # Create a binary array where prediction peak indices are set to 1\n",
    "    predict_prob = np.zeros(window_length)\n",
    "    predict_prob[prediction] = 1\n",
    "\n",
    "    # Convert prediction to a list for easier appending\n",
    "    prediction = list(prediction)\n",
    "\n",
    "    # Detect false positives\n",
    "    for i, index in enumerate(prediction):\n",
    "        print(\"Pred index: \", index)\n",
    "\n",
    "        start_idx = int(max(0, index - neighbourhood_size))\n",
    "        end_idx = int(min(index + neighbourhood_size, window_length - 1))\n",
    "\n",
    "        target_slice = target_prob[start_idx:end_idx + 1]\n",
    "\n",
    "        if not np.any(target_slice == 1):\n",
    "            # Mark as false positive, ensure it appears twice as negative\n",
    "            prediction[i] = -1 * index\n",
    "            prediction.append(-1 * index)\n",
    "\n",
    "    # Detect false negatives\n",
    "    for i, index in enumerate(target):\n",
    "        print(\"Target index: \", index)\n",
    "        start_idx = int(max(0, index - neighbourhood_size))\n",
    "        end_idx = int(min(index + neighbourhood_size, window_length - 1))\n",
    "\n",
    "        prediction_slice = predict_prob[start_idx:end_idx + 1]\n",
    "\n",
    "        if not np.any(prediction_slice == 1):\n",
    "            # Mark as false negative (append only once)\n",
    "            prediction.append(-1 * index)\n",
    "        elif np.sum(prediction_slice) > 1:\n",
    "            # More than one prediction in the neighborhood\n",
    "            indexes = np.where(prediction_slice == 1)[0] + start_idx\n",
    "            distances = np.abs(indexes - index)\n",
    "            nearest_index = np.argmin(distances)\n",
    "\n",
    "            # False positives are the non-nearest ones\n",
    "            false_positives = np.delete(indexes, nearest_index)\n",
    "\n",
    "            for fp in false_positives:\n",
    "                i = prediction.index(fp)\n",
    "                if i < len(prediction):\n",
    "                    prediction[i] = -1 * fp\n",
    "                prediction.append(-1 * fp)  # Append duplicate for false positive\n",
    "\n",
    "        # Convert prediction back to numpy array\n",
    "    return np.sort(np.array(prediction, dtype=int))\n",
    "\n",
    "\n",
    "def calculate_fn_fp_indexes(prediction):\n",
    "    unique, counts = np.unique(prediction[prediction < 0], return_counts=True)\n",
    "    counts_dict = dict(zip(unique, counts))\n",
    "\n",
    "    fn = np.abs(np.array([k for k, v in counts_dict.items() if v == 1]))\n",
    "    fp = np.abs(np.array([k for k, v in counts_dict.items() if v == 2]))\n",
    "\n",
    "    return fn, fp\n",
    "\n",
    "\n",
    "def load_annotations(file_prefix, targets=True, dir=\"./\"):\n",
    "    choice = \"targets\"\n",
    "\n",
    "    if not targets:\n",
    "        choice = \"prediction\"\n",
    "\n",
    "    try:\n",
    "        resp = np.load(os.path.join(dir, str(file_prefix) + f\"_{choice}.npy\"))\n",
    "        with open(os.path.join(dir, str(file_prefix) + f\"_{choice}.peaks\"), \"rb\") as f:\n",
    "            peaks = pickle.load(f)\n",
    "        with open(os.path.join(dir, str(file_prefix) + f\"_{choice}.cycles\"), \"rb\") as f:\n",
    "            cycles = pickle.load(f)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None, None, None\n",
    "\n",
    "    return resp, peaks, cycles\n",
    "\n",
    "def annotate_signal(file_prefix, dir, bpm=\"\", targets=False):\n",
    "\n",
    "    choice = \"prediction\"\n",
    "    if targets:\n",
    "        choice = \"targets\"\n",
    "\n",
    "    file = str(file_prefix) + f\"_{choice}.npy\"\n",
    "\n",
    "    # load signal\n",
    "    resp = np.load(os.path.join(dir, file))\n",
    "\n",
    "    resp = invert_log_softmax(resp)\n",
    "    resp = norm_min_max(resp, -1, 1)\n",
    "    resp = myConv1D(resp, kernel_length=100, padding=\"same\")\n",
    "\n",
    "    samples = [x for x in range(0, len(resp))]\n",
    "\n",
    "    if bpm == \"large\":\n",
    "        peaks, cycles = detect_local_extrema(samples, resp)\n",
    "        peaks = peaks[::2]\n",
    "        cycles = cycles[::2]\n",
    "\n",
    "    elif bpm == \"small\":\n",
    "        # When breaths per minute are small, there is a pause between the expiration of the first cycle and an inspiration of the follow up cycle\n",
    "        # therefore physio is used to calculate the cycles indices as mean index of expiration and inspiration\n",
    "        # and neurokit is used to predict the peaks of resp signal\n",
    "        # other option is to calculate the mean index as the mean of expi + inspi index\n",
    "\n",
    "        _, cycles = physio.compute_respiration(resp, aidmed_resp_sampling_rate, parameter_preset='human_airflow')\n",
    "        inspi_index = cycles['inspi_index'].values\n",
    "        expi_index = cycles['expi_index'].values\n",
    "        cycles = np.sort(np.concatenate((inspi_index, expi_index)))\n",
    "\n",
    "        peaks = nk.rsp_findpeaks(resp, sampling_rate=50)['RSP_Peaks']\n",
    "    else:\n",
    "        raise Exception(\"Wrong bpm parameter. Use either 'large' or 'small'\")\n",
    "\n",
    "    return resp, peaks, cycles\n",
    "\n",
    "\n",
    "def load_postprocess(file_prefix, dir):\n",
    "    target_resp, target_peaks, target_cycles = load_annotations(file_prefix, dir=dir)\n",
    "    predicted_resp, predicted_peaks, predicted_cycles = load_annotations(file_prefix, targets=False, dir=dir)\n",
    "\n",
    "    assert target_resp.shape == predicted_resp.shape\n",
    "\n",
    "    return target_resp, predicted_resp, target_peaks, predicted_peaks, target_cycles, predicted_cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording = 0\n",
    "\n",
    "# annoptating the predictions\n",
    "# target_resp, target_peaks, target_cycles = load_annotations(file_prefix=recording, dir=PATH, targets=True)\n",
    "# predicted_resp, predicted_peaks, predicted_cycles = annotate_signal(file_prefix=recording, dir=PATH, targets=False, bpm=\"large\")\n",
    "\n",
    "# after annotating the files\n",
    "target_resp, target_peaks, target_cycles = load_annotations(file_prefix=recording, dir=POST_PROCESS_PATH, targets=True)\n",
    "predicted_resp, predicted_peaks, predicted_cycles = load_annotations(file_prefix=recording, dir=POST_PROCESS_PATH, targets=False)\n",
    "\n",
    "inputs = np.load(os.path.join(PATH, str(recording) + \"_inputs.npy\"))\n",
    "ecg, rrs, rpa = [inputs[:,:,x].flatten() for x in range(inputs.shape[2])]\n",
    "\n",
    "###### load postprocessed targets and prediction after annotating it with manually + using first/second order derivative or neurokit\n",
    "# target_resp, predicted_resp, target_peaks, predicted_peaks, target_cycles, predicted_cycles = load_postprocess(recording, dir=POST_PROCESS_PATH)\n",
    "\n",
    "t = np.array([x * 1 / aidmed_resp_sampling_rate for x in range(len(target_resp))])\n",
    "t_ecg = np.array([x * 1 / aidmed_ecg_sampling_rate for x in range(len(ecg))])\n",
    "#t_ticks = np.concatenate((t[::aidmed_resp_sampling_rate * 20], [int(len(prediction_resp) * 1 / aidmed_resp_sampling_rate)]), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_resp shape: (11750,)\n",
      "predicted_resp shape: (11750,)\n",
      "target_peaks shape: (33,)\n",
      "predicted_peaks shape: (33,)\n",
      "target_cycles shape: (34,)\n",
      "predicted_cycles shape: (34,)\n",
      "ECG shape: (58750,)\n",
      "RRS shape: (58750,)\n",
      "EPA shape: (58750,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"target_resp shape: {target_resp.shape}\")\n",
    "print(f\"predicted_resp shape: {predicted_resp.shape}\")\n",
    "print(f\"target_peaks shape: {target_peaks.shape}\")\n",
    "print(f\"predicted_peaks shape: {predicted_peaks.shape}\")\n",
    "print(f\"target_cycles shape: {target_cycles.shape}\")\n",
    "print(f\"predicted_cycles shape: {predicted_cycles.shape}\")\n",
    "print(f\"ECG shape: {ecg.shape}\")\n",
    "print(f\"RRS shape: {rrs.shape}\")\n",
    "print(f\"EPA shape: {rpa.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks_to_plot = np.copy(predicted_peaks)\n",
    "cycles_to_plot = np.copy(predicted_cycles)\n",
    "\n",
    "# running second time after fixing peaks and cycles\n",
    "errors_peaks = np.where(predicted_peaks < 0)[0]\n",
    "errors_cycles = np.where(predicted_cycles < 0)[0]\n",
    "\n",
    "if len(errors_peaks) > 0:\n",
    "    fn_peaks, fp_peaks = calculate_fn_fp_indexes(predicted_peaks)\n",
    "    peaks_to_plot = peaks_to_plot[peaks_to_plot > 0]\n",
    "else:\n",
    "    fn_peaks, fp_peaks = [], []\n",
    "\n",
    "\n",
    "if len(errors_cycles) > 0:\n",
    "    fn_cycles, fp_cycles = calculate_fn_fp_indexes(predicted_cycles)\n",
    "    cycles_to_plot = cycles_to_plot[cycles_to_plot > 0]\n",
    "else:\n",
    "    fn_cycles, fp_cycles = [], []\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, sharex=True)\n",
    "\n",
    "ax = axs[0]\n",
    "ax.plot(t_ecg, ecg, color='royalblue')\n",
    "ax.plot(t_ecg, rrs, color='red', linewidth=3)\n",
    "ax.plot(t_ecg, rpa, color='green', linewidth=3)\n",
    "ax.grid()\n",
    "ax.legend([\"ECG\", \"RR intervals\", \"RPA\"])\n",
    "ax.set_title(\"Inputs\", fontweight=\"semibold\")\n",
    "\n",
    "\n",
    "ax = axs[1]\n",
    "ax.plot(t, target_resp, 'k-')\n",
    "ax.scatter(t[target_peaks], target_resp[target_peaks], s=50, color=\"orange\", marker=\"o\")\n",
    "ax.scatter(t[target_cycles], target_resp[target_cycles], s=50, color=\"purple\", marker=\"o\")\n",
    "ax.grid()\n",
    "ax.set_title(\"Target\", fontweight=\"semibold\")\n",
    "ax.legend([\"Resp\", \"Peaks\", \"Cycles\"])\n",
    "#ax.set_xticks(t_ticks)\n",
    "\n",
    "second_legend = []\n",
    "\n",
    "ax = axs[2]\n",
    "ax.plot(t, predicted_resp, 'dimgrey')\n",
    "ax.scatter(t[peaks_to_plot], predicted_resp[peaks_to_plot], s=50, color=\"red\", marker=\"o\")\n",
    "ax.scatter(t[cycles_to_plot], predicted_resp[cycles_to_plot], s=50, color=\"dodgerblue\", marker=\"o\")\n",
    "second_legend += [\"Resp\", \"Peaks\", \"Cycles\"]\n",
    "if len(fn_peaks) > 0:\n",
    "    ax.scatter(t[fn_peaks], predicted_resp[fn_peaks], s=75, color=\"green\", marker=\"X\")\n",
    "    second_legend += [\"FN peaks\"]\n",
    "if len(fp_peaks) > 0:\n",
    "    ax.scatter(t[fp_peaks], predicted_resp[fp_peaks], s=75, color=\"orange\", marker=\"X\")\n",
    "    second_legend += [\"FP peaks\"]\n",
    "if len(fn_cycles) > 0:\n",
    "    ax.scatter(t[fn_cycles], predicted_resp[fn_cycles], s=75, color=\"darkorchid\", marker=\"s\")\n",
    "    second_legend += [\"FN cycles\"]\n",
    "if len(fp_cycles) > 0:\n",
    "    ax.scatter(t[fp_cycles], predicted_resp[fp_cycles], s=75, color=\"violet\", marker=\"s\")\n",
    "    second_legend += [\"FP cycles\"]\n",
    "ax.grid()\n",
    "ax.set_title(\"Predicted\", fontweight=\"semibold\")\n",
    "ax.legend(second_legend)\n",
    "#ax.set_xticks(t_ticks)\n",
    "\n",
    "fig.supxlabel(\"Time [s]\", fontweight=\"semibold\")\n",
    "fig.set_size_inches(8,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing peaks and cycles indices\n",
    "Double negative value - FP\n",
    "Single negative value - FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_peaks = np.concatenate((target_peaks, [265], [5205], [8600]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred index:  252\n",
      "Pred index:  1062\n",
      "Pred index:  1297\n",
      "Pred index:  1604\n",
      "Pred index:  1866\n",
      "Pred index:  2173\n",
      "Pred index:  2506\n",
      "Pred index:  2886\n",
      "Pred index:  3543\n",
      "Pred index:  3990\n",
      "Pred index:  4265\n",
      "Pred index:  4632\n",
      "Pred index:  5055\n",
      "Pred index:  5352\n",
      "Pred index:  5654\n",
      "Pred index:  5885\n",
      "Pred index:  6144\n",
      "Pred index:  6426\n",
      "Pred index:  6627\n",
      "Pred index:  6910\n",
      "Pred index:  7198\n",
      "Pred index:  7845\n",
      "Pred index:  8346\n",
      "Pred index:  8721\n",
      "Pred index:  8849\n",
      "Pred index:  9125\n",
      "Pred index:  9359\n",
      "Pred index:  9574\n",
      "Pred index:  9788\n",
      "Pred index:  10017\n",
      "Pred index:  10290\n",
      "Pred index:  10571\n",
      "Pred index:  10861\n",
      "Pred index:  11135\n",
      "Pred index:  11443\n",
      "Pred index:  -1297\n",
      "Pred index:  -2506\n",
      "Pred index:  -8849\n",
      "Target index:  244\n",
      "Target index:  1037\n",
      "Target index:  1618\n",
      "Target index:  1885\n",
      "Target index:  2154\n",
      "Target index:  2421\n",
      "Target index:  2881\n",
      "Target index:  3553\n",
      "Target index:  3974\n",
      "Target index:  4273\n",
      "Target index:  4618\n",
      "Target index:  5029\n",
      "Target index:  5346\n",
      "Target index:  5634\n",
      "Target index:  5882\n",
      "Target index:  6147\n",
      "Target index:  6396\n",
      "Target index:  6650\n",
      "Target index:  6883\n",
      "Target index:  7218\n",
      "Target index:  7857\n",
      "Target index:  8345\n",
      "Target index:  8693\n",
      "Target index:  8910\n",
      "Target index:  9129\n",
      "Target index:  9366\n",
      "Target index:  9570\n",
      "Target index:  9773\n",
      "Target index:  9987\n",
      "Target index:  10255\n",
      "Target index:  10563\n",
      "Target index:  10838\n",
      "Target index:  11124\n",
      "Target index:  11411\n"
     ]
    }
   ],
   "source": [
    "predicted_peaks = predicted_peaks[predicted_peaks > 0]\n",
    "\n",
    "predicted_peaks = detect_error_indices(target_peaks, predicted_peaks, 50, len(target_resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8910, -8849, -8849, -2506, -2506, -2421, -1297, -1297])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_peaks[predicted_peaks < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred index:  55\n",
      "Pred index:  881\n",
      "Pred index:  1251\n",
      "Pred index:  1460\n",
      "Pred index:  1741\n",
      "Pred index:  2006\n",
      "Pred index:  2288\n",
      "Pred index:  2726\n",
      "Pred index:  3372\n",
      "Pred index:  3813\n",
      "Pred index:  4120\n",
      "Pred index:  4457\n",
      "Pred index:  4880\n",
      "Pred index:  5200\n",
      "Pred index:  5500\n",
      "Pred index:  5746\n",
      "Pred index:  6024\n",
      "Pred index:  6272\n",
      "Pred index:  6532\n",
      "Pred index:  6768\n",
      "Pred index:  7019\n",
      "Pred index:  7667\n",
      "Pred index:  8171\n",
      "Pred index:  8546\n",
      "Pred index:  8798\n",
      "Pred index:  8995\n",
      "Pred index:  9251\n",
      "Pred index:  9452\n",
      "Pred index:  9687\n",
      "Pred index:  9877\n",
      "Pred index:  10130\n",
      "Pred index:  10426\n",
      "Pred index:  10724\n",
      "Pred index:  10992\n",
      "Pred index:  11275\n",
      "Pred index:  11700\n",
      "Pred index:  -881\n",
      "Pred index:  -1251\n",
      "Pred index:  -1460\n",
      "Pred index:  -3372\n",
      "Target index:  63\n",
      "Target index:  682\n",
      "Target index:  1359\n",
      "Target index:  1768\n",
      "Target index:  2018\n",
      "Target index:  2291\n",
      "Target index:  2638\n",
      "Target index:  3236\n",
      "Target index:  3777\n",
      "Target index:  4132\n",
      "Target index:  4440\n",
      "Target index:  4836\n",
      "Target index:  5189\n",
      "Target index:  5504\n",
      "Target index:  5754\n",
      "Target index:  6018\n",
      "Target index:  6278\n",
      "Target index:  6524\n",
      "Target index:  6766\n",
      "Target index:  7026\n",
      "Target index:  7577\n",
      "Target index:  8108\n",
      "Target index:  8544\n",
      "Target index:  8808\n",
      "Target index:  9021\n",
      "Target index:  9252\n",
      "Target index:  9466\n",
      "Target index:  9675\n",
      "Target index:  9873\n",
      "Target index:  10124\n",
      "Target index:  10409\n",
      "Target index:  10705\n",
      "Target index:  10985\n",
      "Target index:  11266\n",
      "Target index:  11615\n"
     ]
    }
   ],
   "source": [
    "# predicted_cycles = predicted_cycles[predicted_cycles > 0]\n",
    "\n",
    "predicted_cycles = detect_error_indices(target_cycles, predicted_cycles, 100, len(target_resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3372, -3372, -3236, -1460, -1460, -1359, -1251, -1251,  -881,\n",
       "        -881,  -682])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_cycles[predicted_cycles < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(POST_PROCESS_PATH + str(recording) + \"_prediction.peaks\", \"wb\") as f:\n",
    "    pickle.dump(predicted_peaks, f)\n",
    "\n",
    "with open(POST_PROCESS_PATH + str(recording) + \"_prediction.cycles\", \"wb\") as f:\n",
    "    pickle.dump(predicted_cycles, f)\n",
    "\n",
    "\n",
    "with open(POST_PROCESS_PATH + str(recording) + \"_targets.peaks\", \"wb\") as f:\n",
    "    pickle.dump(target_peaks, f)\n",
    "\n",
    "with open(POST_PROCESS_PATH + str(recording) + \"_targets.cycles\", \"wb\") as f:\n",
    "    pickle.dump(target_cycles, f)\n",
    "\n",
    "np.save(POST_PROCESS_PATH + str(recording) + \"_targets.npy\", target_resp)\n",
    "np.save(POST_PROCESS_PATH + str(recording) + \"_prediction.npy\", predicted_resp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating metrics using manual annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"E:/ml-data/masters-thesis/myDataset/resp_prediction/postprocess/mixed/\"\n",
    "\n",
    "files = os.listdir(dir)\n",
    "\n",
    "peaks_annotations = [x for x in files if \"prediction.peaks\" in x]\n",
    "cycles_annotations = [x for x in files if \"prediction.cycles\" in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ncc(x, y):\n",
    "    \"\"\"\n",
    "    Computes the normalized cross-correlation coefficient for input signals.\n",
    "    The input signal x can be of shape [batch_size, seq_length] or [seq_length].\n",
    "    The input signal y can be of shape [batch_size, seq_length] or [seq_length].\n",
    "    Returns a mean value of the cross-correlation coefficients.\n",
    "    \"\"\"\n",
    "\n",
    "    # Reshape x and y if they are 1D (seq_length only) to [1, seq_length]\n",
    "    if x.ndim == 1:\n",
    "        x = np.expand_dims(x, 0)  # Shape becomes [1, seq_length]\n",
    "\n",
    "    if y.ndim == 1:\n",
    "        y = np.expand_dims(y, 0)  # Shape becomes [1, seq_length]\n",
    "\n",
    "    # Ensure x and y have the same shape\n",
    "    assert x.shape == y.shape, \"X and Y must be of the same shape\"\n",
    "    \n",
    "    # Compute the mean along the sequence dimension (dim=1)\n",
    "    x_mean = np.mean(x, axis=1, keepdims=True)\n",
    "    y_mean = np.mean(y, axis=1, keepdims=True)\n",
    "\n",
    "    # Subtract the mean from the inputs\n",
    "    x_prime = x - x_mean\n",
    "    y_prime = y - y_mean\n",
    "\n",
    "    # Compute the covariance\n",
    "    covariance = np.sum(x_prime * y_prime, axis=1)\n",
    "\n",
    "    # Compute the standard deviations\n",
    "    std_x = np.sqrt(np.sum(x_prime ** 2, axis=1))\n",
    "    std_y = np.sqrt(np.sum(y_prime ** 2, axis=1))\n",
    "\n",
    "    # Compute the denominator and avoid division by zero\n",
    "    denominator = std_x * std_y\n",
    "    r_xy = np.where(denominator != 0, covariance / denominator, np.zeros_like(covariance))\n",
    "    \n",
    "    # Return the mean of the cross-correlation coefficients\n",
    "    return np.mean(r_xy)\n",
    "\n",
    "\n",
    "def filter_indices(array, filter_values):\n",
    "    valid_indices = [i for i, val in enumerate(array) if val not in np.abs(filter_values)]\n",
    "    return valid_indices\n",
    "\n",
    "\n",
    "def calculate_cycles_data(targets, prediction):\n",
    "    detected_cycles = 0\n",
    "    fp_cycles = 0\n",
    "    fn_cycles = 0\n",
    "\n",
    "    detected_cycles_duration = 0.0\n",
    "    detected_cycles_boundaries_time_deviation = 0.0\n",
    "\n",
    "    # count TP, FP, FN for cycles prediction\n",
    "    true_positive_cycles_indexes = prediction[prediction > 0]\n",
    "\n",
    "    false_negative_cycles_indexes, false_positive_cycles_indexes = calculate_fn_fp_indexes(prediction)\n",
    "\n",
    "    all_errors = np.sort(np.concatenate((false_negative_cycles_indexes, false_positive_cycles_indexes))).astype(int)\n",
    "    all_indexes = np.sort(np.abs(prediction))\n",
    "\n",
    "    if len(all_errors) > 0:\n",
    "        valid_indices = filter_indices(all_indexes, all_errors)\n",
    "        false_negative_indices = [i for i, v in enumerate(all_indexes) if v in false_negative_cycles_indexes]\n",
    "\n",
    "        for i in range(1, len(valid_indices)):\n",
    "            # if indexes are neighbours (checking their position in a list), then there is a cycle\n",
    "            first_indice = valid_indices[i]\n",
    "            second_indice = valid_indices[i-1]\n",
    "\n",
    "            if first_indice - second_indice == 1:\n",
    "                detected_cycles += 1\n",
    "                detected_cycles_duration += abs(all_indexes[first_indice] - all_indexes[second_indice])\n",
    "\n",
    "        if len(false_negative_indices) > 1:\n",
    "            # count false negative cycles\n",
    "            for i in range(1, len(false_negative_indices)):\n",
    "                if false_negative_indices[i] - false_negative_indices[i-1] != 1:\n",
    "                    fn_cycles += 2\n",
    "                else:\n",
    "                    fn_cycles += 1\n",
    "        elif len(false_negative_indices) == 1:\n",
    "            fn_cycles += 2\n",
    "    else:\n",
    "        detected_cycles += len(all_indexes) - 1\n",
    "        detected_cycles_duration += np.sum(np.diff(all_indexes))\n",
    "        \n",
    "    # small respiration rate - false positive cycle influences the correct cycles\n",
    "    # false positive peak indicates a false positive cycle\n",
    "    # therefore instead of one point splitting two ground truth cycles, there are two, indicating start and end of FP cycle\n",
    "    # error will be calculated as the two differences - abs(start of FP cycle - target) and (end - target)\n",
    "\n",
    "    targets = targets[~np.isin(targets, false_negative_cycles_indexes)]\n",
    "\n",
    "    fp_cycles += len(false_positive_cycles_indexes)\n",
    "\n",
    "    detected_cycles_boundaries_time_deviation = np.sum(np.abs(np.sort(true_positive_cycles_indexes) - np.sort(targets)))\n",
    "\n",
    "    return detected_cycles, fp_cycles, fn_cycles, detected_cycles_duration, detected_cycles_boundaries_time_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 0 1 \n",
      "Average peak time deviation:       0.297 [s] | Peak detection rate:           1.0448\n",
      "Average cycle time deviation:      0.435 [s] | Avg cycle detection rate:      0.9851\n",
      "Average detected cycles duration:  6.583 [s] | Average total cycles duration: 6.903 [s]\n",
      "\n",
      "Total recordings duration: 7.83 [min] \n",
      "Total peaks: 67 \n",
      "Avg breaths per minute: 8.55\n",
      "\n",
      "FP peaks count: 3 \n",
      "FN peaks count: 2\n",
      "\n",
      "Total cycles: 67 \n",
      "FP cycles count: 4 \n",
      "FN cycles count: 4\n",
      "\n",
      "Avg NCC: 0.8436\n",
      "Avg L1: 2608.4028\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peaks_time_deviation = 0.0\n",
    "cycles_boundaries_time_deviation = 0.0\n",
    "\n",
    "total_recording_length = 0.0\n",
    "\n",
    "total_peaks = 0\n",
    "detected_peaks = 0\n",
    "fp_detected_peaks = 0\n",
    "fn_detected_peaks = 0\n",
    "\n",
    "total_cycles = 0\n",
    "detected_cycles = 0\n",
    "fp_detected_cycles = 0\n",
    "fn_detected_cycles = 0\n",
    "\n",
    "total_cycles_duration = 0.0\n",
    "detected_cycles_duration = 0.0\n",
    "\n",
    "total_ncc = 0.0\n",
    "total_L1 = 0.0\n",
    "\n",
    "print(\"File:\", end=\" \")\n",
    "for i in range(len(peaks_annotations)):\n",
    "    file_prefix = peaks_annotations[i].split(\"_\")[0]\n",
    "    print(file_prefix, end=\" \")\n",
    "\n",
    "    # load target resp and annotations\n",
    "    target_resp, predicted_resp, target_peaks, predicted_peaks, target_cycles, predicted_cycles = load_postprocess(file_prefix, dir=dir) # for fast_breathing and normal_breathing\n",
    "\n",
    "    total_peaks += len(target_peaks)                                            # total number of peaks\n",
    "    total_cycles += len(target_cycles) - 1                                      # total number of cycles\n",
    "    total_cycles_duration += np.sum(np.diff(np.sort(target_cycles)))            # total duration of cycles in samples\n",
    "    total_recording_length += len(target_resp) / aidmed_resp_sampling_rate / 60 # total duration of recordings in minutes\n",
    "\n",
    "    # count TP, FP, FN for peaks prediction\n",
    "    true_positive_peaks = predicted_peaks[predicted_peaks > 0]\n",
    "    unique, counts = np.unique(predicted_peaks[predicted_peaks < 0], return_counts=True)\n",
    "    counts_dict = dict(zip(unique, counts))\n",
    "\n",
    "    false_negative_peaks = np.abs(np.array([k for k, v in counts_dict.items() if v == 1]))\n",
    "    fn_detected_peaks += len(false_negative_peaks)\n",
    "\n",
    "    false_positive_peaks = np.abs(np.array([k for k, v in counts_dict.items() if v == 2]))\n",
    "    fp_detected_peaks += len(false_positive_peaks)\n",
    "\n",
    "    # Handle prediction errors (negative values)\n",
    "    peaks_errors_indexes = predicted_peaks[predicted_peaks < 0]\n",
    "    cycles_errors_indexes = predicted_cycles[predicted_cycles < 0]\n",
    "\n",
    "    detected_peaks += len(np.unique(predicted_peaks))\n",
    "    # remove error peaks indices from the predictions\n",
    "    if len(peaks_errors_indexes) > 0:\n",
    "        predicted_peaks = predicted_peaks[~np.isin(predicted_peaks, peaks_errors_indexes)]\n",
    "        target_peaks = target_peaks[~np.isin(target_peaks, np.abs(peaks_errors_indexes))]\n",
    "        \n",
    "    # Calculate time deviation between predicted and target peaks\n",
    "    peaks_time_deviation += np.sum(np.abs(np.sort(target_peaks) - np.sort(predicted_peaks)))\n",
    "\n",
    "    dc, fp, fn, dcd, dcbtd = calculate_cycles_data(target_cycles, predicted_cycles)\n",
    "\n",
    "    detected_cycles += dc\n",
    "    fp_detected_cycles += fp\n",
    "    fn_detected_cycles += fn\n",
    "    detected_cycles_duration += dcd\n",
    "    cycles_boundaries_time_deviation += dcbtd\n",
    "\n",
    "    total_ncc += ncc(target_resp, predicted_resp)\n",
    "    total_L1 += np.sum(np.abs(target_resp - predicted_resp))\n",
    "\n",
    "avg_peak_time_deviation = peaks_time_deviation / detected_peaks / aidmed_resp_sampling_rate if detected_peaks > 0 else 0\n",
    "avg_peak_detection_rate = detected_peaks / total_peaks if total_peaks > 0 else 0\n",
    "\n",
    "avg_cycle_time_deviation = cycles_boundaries_time_deviation / detected_cycles / aidmed_resp_sampling_rate if detected_cycles > 0 else 0\n",
    "avg_cycle_detection_rate = (detected_cycles + fp_detected_cycles) / total_cycles if detected_cycles > 0 else 0\n",
    "\n",
    "avg_detected_cycles_duration = detected_cycles_duration / detected_cycles / aidmed_resp_sampling_rate if detected_cycles > 0 else 0\n",
    "avg_total_cycles_duration = total_cycles_duration / total_cycles / aidmed_resp_sampling_rate if total_cycles > 0 else 0\n",
    "\n",
    "print(f\"\\nAverage peak time deviation: {avg_peak_time_deviation:>11.3f} [s] | Peak detection rate: {avg_peak_detection_rate:>16.4f}\\n\"\n",
    "      f\"Average cycle time deviation: {avg_cycle_time_deviation:>10.3f} [s] | Avg cycle detection rate: {avg_cycle_detection_rate:>11.4f}\\n\"\n",
    "      f\"Average detected cycles duration: {avg_detected_cycles_duration:>6.3f} [s] | Average total cycles duration: {avg_total_cycles_duration:>5.3f} [s]\\n\"\n",
    "      f\"\\nTotal recordings duration: {total_recording_length:.2f} [min] \\nTotal peaks: {total_peaks} \\nAvg breaths per minute: {total_peaks / total_recording_length:.2f}\\n\"\n",
    "      f\"\\nFP peaks count: {fp_detected_peaks} \\nFN peaks count: {fn_detected_peaks}\\n\"\n",
    "      f\"\\nTotal cycles: {total_cycles} \\nFP cycles count: {fp_detected_cycles} \\nFN cycles count: {fn_detected_cycles}\\n\"\n",
    "      f\"\\nAvg NCC: {total_ncc / len(peaks_annotations):.4f}\\n\"\n",
    "      f\"Avg L1: {total_L1 / len(peaks_annotations):.4f}\\n\"\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
